{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from nltk.tag import pos_tag #For proper noun\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-processing the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"C:\\\\Users\\\\ayush\\\\Desktop\\\\TIET - 5th Sem 2020-21\\\\NLP\\\\COVID_19_dataset\\\\documents\\\\001.txt\"\n",
    "f = open(filename, 'r')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all words to lower case and removing stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "word_tokens_lower = [word.lower() for word in word_tokens]\n",
    "stopWords = list(set(stopwords.words(\"english\")))\n",
    "word_tokens_refined = [x for x in word_tokens_lower if x not in stopWords]\n",
    "print(len(word_tokens_refined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "stem = []\n",
    "ps = PorterStemmer()\n",
    "for word in word_tokens_refined:\n",
    "    stem.append(ps.stem(word))\n",
    "word_tokens_refined = stem\n",
    "print(len(word_tokens_refined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction \n",
    "A. Sentence Features: \n",
    " 1. Cue-Phrases like example, therefore, important, according to, etc.\n",
    " 2. Numerical Data like dates, transactions, year, age, etc.\n",
    " 3. Sentence Length like too long or too short sentence are of little worth\n",
    " 4. Sentene Position like starting and ending sentences are of more importance\n",
    "\n",
    "B. Word Features: \n",
    " 1. Word Frequency\n",
    " 2. Upper Case\n",
    " 3. Proper Noun\n",
    " 4. Heading Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1. Cue Phrases Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n"
     ]
    }
   ],
   "source": [
    "QPhrases = ['examples','anyway','furhtermore','first','second','then','now','therefore','hence','lastly','finally','summary']\n",
    "cuePhrases = {}\n",
    "for sentence in sent_tokens:\n",
    "    cuePhrases[sentence] = 0\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    for word in word_tokens:\n",
    "        if word.lower() in QPhrases:\n",
    "            cuePhrases[sentence] += 1\n",
    "maximumFreq = max(cuePhrases.values())\n",
    "for k in cuePhrases.keys():\n",
    "    try:\n",
    "        cuePhrases[k] = cuePhrases[k]/maximumFreq\n",
    "        cuePhrases[k] = round(cuePhrases[k],3)\n",
    "    except ZeroDivisionError:\n",
    "        x = 0\n",
    "print(cuePhrases.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Numerical Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "numericData = {}\n",
    "for sentence in sent_tokens:\n",
    "    numericData[sentence] = 0\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    for word in word_tokens:\n",
    "        if word.isdigit():\n",
    "            numericData[sentence] += 1\n",
    "maximumFreq = max(numericData.values())\n",
    "for k in numericData.keys():\n",
    "    try:\n",
    "        numericData[k] = numericData[k]/maximumFreq\n",
    "        numericData[k] = round(numericData[k],3)\n",
    "    except ZeroDivisionError:\n",
    "        x = 0\n",
    "print(numericData.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3. Sentence Length Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.8, -2.25, 1, -0.1, 1, 1, 0.6, 0.35, 1, 0.85])\n"
     ]
    }
   ],
   "source": [
    "# if sentence is less than 10 words, reduce 5% value of that sentence\n",
    "# if sentence is 10 to 20 words, keep maimum weight i.e. 1\n",
    "# if sentence is greater than 20 words, reduce 5% value of that sentence\n",
    "\n",
    "sentLenScore = {}\n",
    "for sentence in sent_tokens:\n",
    "    sentLenScore[sentence] = 0\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    if len(word_tokens) < 10:\n",
    "        sentLenScore[sentence] = 1 - 0.05*(10-len(word_tokens))\n",
    "        sentLenScore[sentence] = round(sentLenScore[sentence],4)\n",
    "    elif len(word_tokens) > 20:\n",
    "        sentLenScore[sentence] = 1 - 0.05*(len(word_tokens)-20)\n",
    "        sentLenScore[sentence] = round(sentLenScore[sentence],4)\n",
    "    else:\n",
    "        sentLenScore[sentence] = 1\n",
    "print(sentLenScore.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4. Sentence Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.2, 0.25, 0.333, 0.5, 1.0])\n"
     ]
    }
   ],
   "source": [
    "sentencePosition = {}\n",
    "d = 1 #Sentence number\n",
    "no_of_sentences = len(sent_tokens)\n",
    "for i in range(no_of_sentences):\n",
    "    a = 1/d\n",
    "    b = 1/(no_of_sentences-d+1)\n",
    "    sentencePosition[sent_tokens[d-1]] = max(a,b)\n",
    "    sentencePosition[sent_tokens[d-1]] = round(sentencePosition[sent_tokens[d-1]],3)\n",
    "    d = d+1\n",
    "print(sentencePosition.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1 Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.602, 0.477, 0.602, 0.602, 0.903, 0.477, 0.602, 0.301, 0.301, 0.477, 0.778, 0.699, 0.699, 0.602, 1.146, 0.477, 0.778, 0.602, 0.301, 1.041, 0.301, 0.477, 0.301, 0.301, 0.301, 0.477, 0.477, 0.699, 0.477, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.477, 0.477, 0.602, 0.301, 0.477, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.477, 0.477, 0.301, 0.301, 0.301, 0.477, 0.602, 0.477, 0.477, 0.301, 0.301, 0.301, 0.301, 0.477, 0.301, 0.301, 0.477, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.477, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.477, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.477, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301, 0.301])\n"
     ]
    }
   ],
   "source": [
    "freqTable = {}\n",
    "for word in word_tokens_refined:\n",
    "    if word in freqTable:\n",
    "        freqTable[word] += 1\n",
    "    else:\n",
    "        freqTable[word] = 1\n",
    "for word in freqTable.keys():\n",
    "    freqTable[word] = math.log10(freqTable[word]+1)\n",
    "    freqTable[word] = round(freqTable[word],3)\n",
    "print(freqTable.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([12.467000000000002, 24.43299999999999, 5.801, 12.745000000000005, 6.151000000000002, 4.829000000000001, 9.337000000000002, 7.510000000000001, 6.260000000000001, 6.929000000000001])\n"
     ]
    }
   ],
   "source": [
    "#Calculate sentence score according to word frequency\n",
    "wordFreq = {}\n",
    "for sentence in sent_tokens:\n",
    "    wordFreq[sentence] = 0\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    f = []\n",
    "    for word in word_tokens:\n",
    "        f.append(ps.stem(word))\n",
    "    for word,freq in freqTable.items():\n",
    "        if word in f:\n",
    "            wordFreq[sentence] += freq\n",
    "print(wordFreq.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Upper Case Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "upperCase = {}\n",
    "for sentence in sent_tokens:\n",
    "    upperCase[sentence] = 0\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    for word in word_tokens:\n",
    "        if k.isupper():\n",
    "            upperCase[sentence] += 1\n",
    "maxFreq = max(upperCase.values())\n",
    "for k in upperCase.keys():\n",
    "    try:\n",
    "        upperCase[k] = upperCase[k]/maxFreq\n",
    "        upperCase[k] = round(upperCase[k],3)\n",
    "    except ZeroDivisionError:\n",
    "        x = 0\n",
    "print(upperCase.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 Proper Noun Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.364, 1.0, 0.0, 0.909, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0])\n"
     ]
    }
   ],
   "source": [
    "properNoun = {}\n",
    "for sentence in sent_tokens:\n",
    "    tagged_sent = pos_tag(sentence.split())\n",
    "    propernouns = [word for word,pos in tagged_sent if pos=='NNP']\n",
    "    properNoun[sentence] = len(propernouns)\n",
    "maxFreq = max(properNoun.values())\n",
    "for k in properNoun.keys():\n",
    "    try:\n",
    "        properNoun[k] = properNoun[k]/maxFreq\n",
    "        properNoun[k] = round(properNoun[k],3)\n",
    "    except ZeroDivisionError:\n",
    "        x = 0\n",
    "print(properNoun.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4 Heading Match Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.87, 1.0, 0.304, 0.478, 0.13, 0.174, 0.13, 0.13, 0.261, 0.217])\n"
     ]
    }
   ],
   "source": [
    "headMatch = {}\n",
    "heading = sent_tokens[0]\n",
    "for sentence in sent_tokens:\n",
    "    headMatch[sentence] = 0\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    for k in word_tokens:\n",
    "        if k not in stopWords:\n",
    "            k = ps.stem(k)\n",
    "            if k in ps.stem(heading):\n",
    "                headMatch[sentence] += 1\n",
    "maxFreq = max(headMatch.values())\n",
    "for k in headMatch.keys():\n",
    "    try:\n",
    "        headMatch[k] = headMatch[k]/maxFreq\n",
    "        headMatch[k] = round(headMatch[k],3)\n",
    "    except ZeroDivisionError:\n",
    "        x = 0\n",
    "print(headMatch.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling all the features to get the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([15.501000000000003, 24.68299999999999, 7.438000000000001, 15.282000000000005, 7.572000000000002, 6.203000000000001, 10.317000000000002, 8.323000000000002, 8.021, 9.996000000000002])\n"
     ]
    }
   ],
   "source": [
    "totalScore = {}\n",
    "for k in cuePhrases.keys():\n",
    "    totalScore[k] = cuePhrases[k] + numericData[k] + sentLenScore[k] + sentencePosition[k] + wordFreq[k] + upperCase[k] + properNoun[k] + headMatch[k]\n",
    "print(totalScore.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumValues = 0\n",
    "for sentence in totalScore:\n",
    "    sumValues += totalScore[sentence]\n",
    "average = sumValues/len(totalScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fact that two coronavirus vaccines recently showed strong protection against COVID-19 bodes well for other leading programs led by AstraZeneca, Novavax, and Johnson & Johnson, Bill Gates said Tuesday.The billionaire Microsoft founder and philanthropist said it will be easier to boost manufacturing and distribute these other shots to the entire world, particularly developing nations.The vaccine space has seen a flurry of good news in recent days, marked by overwhelming success in late-stage trials by both Pfizer and Moderna.\n"
     ]
    }
   ],
   "source": [
    "# Storing sentences into summary\n",
    "summary = ''\n",
    "for sentence in sent_tokens:\n",
    "    if sentence in totalScore and totalScore[sentence] > (1.5*average):\n",
    "        summary += \"\"+sentence\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
